# DATA IMPORT
# library imports
import nltk
import pandas as pd
import re
import string
import matplotlib.pyplot as plt
import collections
import spacy
 
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
from nltk.stem.porter import *
from nltk.corpus import wordnet
from nltk.stem.wordnet import WordNetLemmatizer
from nltk import pos_tag
from pywsd.utils import lemmatize_sentence
from collections import Counter
from wordcloud import WordCloud
from lexical_diversity import lex_div as ld
from yellowbrick.text import PosTagVisualizer
from textblob import Word

# reading in text file, containing our excerpt. then we can open it up and see what we've got...
with open('cookbook_excerpt.txt', 'r') as file:
    cookbook_excerpt = file.read().replace('\n', '')
    
# let's open this up and see what it looks like! 
cookbook_excerpt

# DATA CLEANING & PRE-PROCESSING
# convert text to lowercase 
cookbook_excerpt_stripped = cookbook_excerpt.lower() 

# getting rid of punctuation 
cookbook_excerpt_stripped = cookbook_excerpt_stripped.strip(string.punctuation) 

# strip text of non-alphanumerics 
cookbook_excerpt_stripped = re.sub(r'\W+', ' ', cookbook_excerpt_stripped) 

# let's open this up and see what it looks like! 
cookbook_excerpt_stripped

# the word_tokenize() function in NLTK splits strings into words based on whitespace and punctuation. # in this case, we've already stripped our text of punctuation so the process of tokenization is based off the former 
cookbook_excerpt_tokens = nltk.word_tokenize(cookbook_excerpt_stripped) 

# let's open it up and see what it looks like! 
cookbook_excerpt_tokens

# printing the list of stopwords we'll be working to remove 
print(stopwords.words('english'))

# removal of stopwords 
cookbook_excerpt_cleaned = [word for word in cookbook_excerpt_tokens if not word in stopwords.words('english')] 
# let's open this up and see what it looks like! 
cookbook_excerpt_cleaned

# attempt at stemming 
porter_stemmer = PorterStemmer() 
for word in cookbook_excerpt_cleaned: 
    print(word+' -->'+porter_stemmer.stem(word))
    
# initialize an empty string 
def listToString(s):  
    str1 = " " 
    
    # return string   
    return (str1.join(s))

cookbook_excerpt_cleaned_str = listToString(cookbook_excerpt_cleaned) 

# check to see what what conversion created
cookbook_excerpt_cleaned_str

cookbook_excerpt_lemmatized =lemmatize_sentence(cookbook_excerpt_cleaned_str)
 
# let's open it up and see if we did it! 
cookbook_excerpt_lemmatized

# DATA EXPLORATION
dist = nltk.FreqDist(cookbook_excerpt_lemmatized) 
for word, frequency in dist.most_common(50): 
    print(u'{};{}'.format(word, frequency))
    
word_freq_dict = dict(dist) 
words_to_print = int(input("How many most common words to print: ")) 

print("\nOK. The {} most common words are as follows\n".format(words_to_print)) 

word_counter = collections.Counter(word_freq_dict) 

for word, count in word_counter.most_common(words_to_print): 
    print(word, ": ", count)

# initialize an empty string 
def listToString(s):  
    str1 = " " 
    
    # return string   
    return (str1.join(s))

cookbook_excerpt_lemmatized_str = listToString(cookbook_excerpt_lemmatized) 

# check to see what what conversion created
cookbook_excerpt_lemmatized_str

lst = word_counter.most_common(words_to_print) 
wordcount_df = pd.DataFrame(lst, columns = ['Word', 'Count']) 
wordcount_df.plot.bar(x='Word',y='Count', title = "Most Commonly Used Words")
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110, background_color = "white", max_words = 40).generate(cookbook_excerpt_lemmatized_str)

# word cloud
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

# lexical diversity
# text-type ratio calculated using the lexical_diversity package 
ld.ttr(cookbook_excerpt_lemmatized)

# remember! we're reusing our cookbook_excerpt_tokens variable from earlier! 
cookbook_excerpt_pos_tags = nltk.pos_tag(cookbook_excerpt_tokens)

# create dataframe to hold the list; this will make for easier visualization
pos_df = pd.DataFrame(cookbook_excerpt_pos_tags)
pos_df.columns = ['token', 'part_of_speech_tag']
#pos_df.head()

# create a count value and add to dataframe 
pos_count = pos_df['part_of_speech_tag'].value_counts()
type(pos_count) # currently series, need to turn to dataframe
pos_count_df = pos_count.to_frame() 
pos_count_df.head()

# visualize distribution of part of speech (bar plot version of chart above)
pos_count_df.plot(kind = 'bar', title = "Distribution of Part of Speech Tags in Our Excerpt")
plt.xlabel('part of speech tag')
plt.ylabel('count')
plt.show()
